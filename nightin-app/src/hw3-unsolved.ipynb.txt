{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"top\"></div> \n",
    "# Table of contents\n",
    "* <a href='#Submission-instructions'>Submission instructions</a>\n",
    "* <a href='#Background'>Description of HW3</a>\n",
    "* <a href=\"#Task\">Start of this homework</a>\n",
    "\n",
    "# Submission instructions\n",
    "\n",
    "See instructions on Sakai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The Federalist Papers is a collection of 85 articles written under pseudonym Publius. The authors of these articles were Alexander Hamilton, James Madison and John Jay. A number of these articles are not clearly attributable to an author. Further, some articles were products of collaboration between authors. \n",
    "\n",
    "## Task\n",
    "We will try to attribute articles to authors. We are going to cluster the articles. To accomplish this, we will use two models.\n",
    "\n",
    "1. We will model the word counts using mixture of product of Poissons. For each cluster and word combination we will have a single poisson distribution. \n",
    "2. We will model transformed word counts using mixture of multivariate Gaussians with diagonal covariance.\n",
    "\n",
    "We will use articles with known authors to assess our clusters. Specifically, we will perform a statistical test for each cluster and each author that will tell us how likely is it that articles by the same author end up in the cluster by chance. The null hypothesis -- that we are trying to reject -- is that the assignment of articles to clusters is completely random and there is no association between our clusters and authors of articles. We compute probability that the assignment of authors to clusters is generated randomly. If this probability is low, then we can reject the null hypothesis and claim that association between authors and clusters is significant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#load data\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "    kwargs = {}\n",
    "except:\n",
    "    import _pickle as pickle\n",
    "    kwargs = {'encoding':'bytes'}\n",
    "import gzip\n",
    "import scipy\n",
    "with gzip.open(\"preprocessed_documents.pgz\",\"rb\") as f:\n",
    "    #documents, counter = pickle.load(f, **kwargs)\n",
    "    documents, counter = pd.read_pickle(f)\n",
    "    \n",
    "dataMat = scipy.sparse.vstack([doc['acounts'] for doc in documents])\n",
    "dataMat = np.asarray( dataMat.todense().astype('int32') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use counts of functional words as features for clustering the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of used features: 199, they are ['about' 'above' 'accordingly' 'after' 'against' 'all' 'along' 'also'\n",
      " 'although' 'amidst' 'among' 'amongst' 'an' 'and' 'another' 'anti' 'any'\n",
      " 'anything' 'are' 'around' 'as' 'aside' 'at' 'bar' 'be' 'because' 'been'\n",
      " 'before' 'behind' 'below' 'beneath' 'besides' 'between' 'beyond' 'both'\n",
      " 'but' 'by' 'can' 'certain' 'concerning' 'consequently' 'considering'\n",
      " 'could' 'dare' 'do' 'down' 'during' 'each' 'either' 'enough' 'even'\n",
      " 'every' 'everything' 'except' 'excluding' 'failing' 'few' 'fewer'\n",
      " 'following' 'for' 'from' 'given' 'had' 'has' 'have' 'he' 'hence' 'her'\n",
      " 'hers' 'herself' 'him' 'himself' 'his' 'however' 'if' 'in' 'including'\n",
      " 'inside' 'into' 'is' 'it' 'its' 'itself' 'less' 'like' 'little' 'many'\n",
      " 'may' 'me' 'might' 'mine' 'more' 'most' 'much' 'must' 'my' 'myself' 'near'\n",
      " 'neither' 'nevertheless' 'no' 'none' 'nor' 'not' 'nothing'\n",
      " 'notwithstanding' 'now' 'of' 'off' 'on' 'once' 'one' 'only' 'opposite'\n",
      " 'or' 'other' 'ought' 'our' 'ours' 'ourselves' 'out' 'over' 'part' 'past'\n",
      " 'per' 'regarding' 'respecting' 'round' 'save' 'saving' 'several' 'shall'\n",
      " 'she' 'should' 'since' 'so' 'some' 'something' 'such' 'than' 'that' 'the'\n",
      " 'their' 'theirs' 'them' 'themselves' 'then' 'thence' 'there' 'therefore'\n",
      " 'these' 'they' 'things' 'this' 'those' 'though' 'through' 'throughout'\n",
      " 'thus' 'till' 'to' 'toward' 'towards' 'under' 'unless' 'unlike' 'until'\n",
      " 'up' 'upon' 'us' 'various' 'wanting' 'was' 'we' 'were' 'what' 'whatever'\n",
      " 'when' 'whenever' 'where' 'whereas' 'wherever' 'whether' 'which' 'while'\n",
      " 'whilst' 'who' 'whoever' 'whom' 'whose' 'will' 'with' 'within' 'without'\n",
      " 'would' 'yet' 'you' 'your' 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# load functional the word list\n",
    "import codecs\n",
    "with codecs.open('updated_functionWords.txt') as f:\n",
    "    function_words = f.read().splitlines() \n",
    "# find interaction between the list and the words in the documents.\n",
    "func_lst = np.nonzero(np.in1d(counter.get_feature_names(),(function_words)))[0]\n",
    "lst = func_lst\n",
    "words = np.array(counter.get_feature_names())[lst]\n",
    "print( '# of used features: {}, they are {}'.format( len(lst), words ))\n",
    "dataMat_selected = np.asarray(dataMat[:,lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Poissons \n",
    "We will assume that each article belongs to a single cluster.\n",
    "Since we are modeling word counts, a natural modeling choice is to assume that counts are distributed according to the Poisson distribution. Each cluster will have a Poisson distribution associated with each word.\n",
    "\n",
    "We will derive the formulation of mixture of Poissons model and fit it to the data. <br \\><br \\>\n",
    "1) Possion pmf $$p(k \\mid \\lambda) = \\frac{\\lambda^{k} e^{-\\lambda}}{k!}$$ tells us probability of observing a count of $k$ for specific $\\lambda$ value <br \\><br \\>\n",
    "\n",
    "2) Notation for the model\n",
    "  1. $x_i$ be the feature vector of the $i^{th}$ sample, $x_{i, j}$ be the $j^{th}$ features of the $i^{th}$ sample. \n",
    "  2. $h_i$ be the index of the cluster for the $i^{th}$ sample. \n",
    "  3. $\\lambda_m$ be the lambda vector for the $m^{th}$ cluster, $\\lambda_{m,j}$ be the parameters of Possion pmf for cluster c and feature j.\n",
    "  4. $p(h_i = c) = \\pi_c$\n",
    "  5. $p(x_i \\mid h_i = m, \\lambda) = \\prod_j p(x_{i,j}\\mid \\lambda_{m,j})$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **(1/2pt)** Write out log probability of sample $x_i$ given that it belong to cluster $m$, ($h_i = m$):\n",
    "\n",
    "$$\\log p(x_i| h_i = m, \\lambda) = \\sum_j  ... $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **(1/2pt)** Write out log of marginal probability of sample $x_i$ in terms of $p(x_i\\mid\\lambda,h_i)$ and $\\pi$\n",
    "$$\n",
    "\\log p(x_i \\mid \\lambda,\\pi) = \\log \\sum_c ...\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **(1pt)** Write out log-likelihood using the above log-probability for all samples\n",
    "\n",
    "\\begin{aligned}\n",
    "LL(\\lambda,\\pi) &= \\log( \\prod_{i} p(x_{i}))\\\\\n",
    "&= \\sum_{i} ..\\\\\n",
    "& = \\sum_{i}  ...\\\\\n",
    "& = \\sum_{i} ...\\\\\n",
    "\\end{aligned}\n",
    "<br \\><br \\>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) **(1pt)** Apply Jensen’s inequality to derive a lower-bound.\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "LL(\\lambda, \\pi) = \\log( \\prod_{i} p(x_{i})) &= \\sum_{i} \\log \\left \\{ \\sum_{c} q(h_i=c) \\frac{ p(x_{i}, h_i = c) }{q(h_i=c)} \\right \\} \\\\\n",
    "& \\ge \\sum_{i} ... \\log \\left\\{ ... \\right\\} \\\\\n",
    "&= \\sum_{i} ... \\log \\left\\{  ... \\right\\} - \\sum_{i} ...  \\log \\left\\{ ... \\right\\}\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>\n",
    "\n",
    "To check your answer, plug-in $p(h_i=c \\mid x_i)$ in place of $q(h_i=c)$. You should be recover log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **(1pt)** If we let $q(h_i)$ be the posterior probability $p(h_i\\mid x_i,\\lambda, \\pi)$, we maximize the lower-bound. Use Bayes rule to derive posterior\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "p(h_i = m\\mid x_i, \\lambda, \\pi) &= \\frac{...}{...} \\\\\n",
    "&= \\frac{...}{...}\\\\\n",
    "&= \\frac{...}{...}\\\\\n",
    "&= \\frac{...}{...}\\\\\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **(1pt)** Implement function that computes log probability of a single sample. Note that for computing log factorial you need to use the function we provide. <br \\>\n",
    "  Due to numerical precision, we compute probability in log domain. First implement the function to compute $\\log p(x_i \\mid \\lambda_m)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logsum(lp):\n",
    "    m = np.max(lp)    \n",
    "    return np.log(np.sum(np.exp(lp-m))) + m\n",
    "\n",
    "#you cannot compute the factorial directly for large x, compute it in log domain\n",
    "def logfactorial(x):    \n",
    "    return np.sum(np.log(np.arange(1,x+1)))\n",
    "\n",
    "#xs: a vector for x_i\n",
    "#ls: a vector for lambdas\n",
    "#return log probability of equation \n",
    "def logprobvec(xs,ls, compute_factorial = False): \n",
    "    logfactorial_val = np.zeros((len(xs, )))\n",
    "    if compute_factorial:\n",
    "        for i in np.arange(len(xs)):\n",
    "            logfactorial_val[i] = logfactorial(xs[i])\n",
    "    lp = ... - logfactorial_val\n",
    "    lp = np.sum(lp)\n",
    "    return lp\n",
    "\n",
    "#test function\n",
    "test_x = np.array([5, 33, 211, 474])\n",
    "test_l = np.array([4, 60, 300, 600])\n",
    "res_v = logprobvec(test_x, test_l, False)\n",
    "assert( np.allclose( res_v, 3413.687601 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) **(1pt)** Implement a function that computes log posterior for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xs, an array of shape N*F (N, # samples, F, # features)\n",
    "#lambdas, an array of shape K*F (K, # clusters, F, # features )\n",
    "#pis, a vector of shape K\n",
    "#return three varaibles:\n",
    "#logprobs: an array of shape K*N, the log posterior probabilities in 7), the log probability of a sample belong to each cluster.\n",
    "#loglik: log-likelihood of the model in 4).\n",
    "#labels: a vector of shape N, the most probable cluster each sample belongs to.\n",
    "def logposterior_MP(xs,lambdas,pis):\n",
    "    K = lambdas.shape[0]    \n",
    "    F = lambdas.shape[1]\n",
    "    N = xs.shape[0]    \n",
    "    assert(xs.shape[1] == F)\n",
    "    logprobs = np.zeros((K,N))\n",
    "    loglik = 0\n",
    "    labels = np.zeros(N)\n",
    "    for n in range(N) :\n",
    "        x = xs[n,:]\n",
    "        for k in range(K):\n",
    "            ls = lambdas[k,:]\n",
    "            logprobs[k,n] = ...      \n",
    "        docloglik = logsum(logprobs[:,n])    \n",
    "        loglik = loglik + docloglik\n",
    "        logprobs[:,n] = ...     \n",
    "        labels[n] = np.argmax(logprobs[:,n])\n",
    "    \n",
    "    return logprobs, loglik, labels\n",
    "\n",
    "#test function\n",
    "test_x = np.array( [[7, 4, 6], [2, 8, 1], [3, 3, 9]], dtype='float32' )\n",
    "test_l = np.tile( np.round(np.mean(test_x, axis=0)), (3,1))\n",
    "np.random.seed(10)\n",
    "test_l = test_l + 1 + np.abs(np.random.randn(test_l.shape[0], test_l.shape[1]))*2\n",
    "test_pis = np.array([0.33, 0.33, 0.33])\n",
    "res_lp, res_ll, res_lab = logposterior( test_x, test_l, test_pis )\n",
    "assert( np.allclose( res_lp, [[-1.57270506, -4.35048081, -2.08895623],\n",
    "       [-1.3579241 , -1.1180958 , -0.75511075],\n",
    "       [-0.62488554, -0.41521594, -0.90084777]] ) )\n",
    "assert( np.allclose( res_ll, 21.969895 ) )\n",
    "assert( np.allclose( res_lab, [2, 2, 1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **(1pt)** Derive the update formulation for $\\lambda_{m, j}$. Take the derivative of the lower-bound on log-likelihood.\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i} \\sum_{j} \\sum_{c} q(h_i=c) \\log \\left \\{  p(x_{i,j}, h_i = c) \\right \\} &=\n",
    "....  \\\\\n",
    "&=\n",
    "....  \\\\\n",
    "&=\n",
    "....  \\\\\n",
    "&=\n",
    "....  \\\\\n",
    "&= \\sum_{i}  q(h_i=m) \\left( ... \\right)\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>\n",
    "Let the derivative be zero, we have\n",
    "$$\n",
    "\\lambda_{m, j} = \\frac{ \\sum_i q(h_i=m) ... }{\\sum_i q(h_i=m) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) **(1pt)** Derive the update formulation for $\\pi_m$. Take the derivative of the lower-bound. (Note: $\\sum_c \\pi_c = 1$)\n",
    "<br \\> <br \\> $$\n",
    "    \\text{Lagrangian}\\> L(\\pi, \\gamma) = \\sum_{i}  q(h_i=m) \\log \\left\\{ \\pi_m \\right\\} + \\gamma(\\sum_c \\pi_c - 1 )\n",
    "$$ <br \\><br \\>\n",
    "Take deriviate on both $\\pi_m$ and $\\gamma$ and set them to zero, we have\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "    &\\sum_i \\frac{q(h_i=m)}{\\pi_m} + \\gamma = 0 \\\\\n",
    "    &\\sum_c \\pi_c= 1\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>\n",
    "From above, we have\n",
    "<br \\> <br \\> $$\n",
    "\\pi_m = ...\n",
    "$$ <br \\><br \\>\n",
    "where N is the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) **(1pt)**  Write the code to update $\\lambda_{m,j}$ and $\\pi_m$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qs: an array of shape K*N, the posterior probabilities in 7), the probability of a sample belong to each cluster.\n",
    "#return two variables:\n",
    "#ls, the updated lambda, an array of shape K*F (K, # clusters, F, # features )\n",
    "#pis: the updated pi, an vector of shape K\n",
    "def update_MP(qs,xs):\n",
    "    suff = ...\n",
    "    # we add small constant to avoid division by zero\n",
    "    ls = (suff+0.001)/(0.001+np.sum(qs,1)[:,np.newaxis])\n",
    "    pis = ...\n",
    "    pis = .../...\n",
    "    return ls,pis\n",
    "\n",
    "#test function\n",
    "test_qs = np.array([[.9, .3],\n",
    "       [.05, .4],\n",
    "       [.05, .3]])\n",
    "test_xs = np.array( [[7, 4, 6], [2, 8, 1]], dtype='float32' )\n",
    "res_ls, res_pis = update_MP(test_qs, test_xs)\n",
    "assert( np.allclose( res_ls,[[ 5.746044,  4.996669,  4.74687 ],\n",
    "       [ 2.552106, 7.541019,  1.554323],\n",
    "       [ 2.709401,  7.410256,  1.712250]] ))\n",
    "assert( np.allclose( res_pis, [0.6, 0.225, 0.175] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "13)  Run the code below, which will invoke your E-step (```logposterior_MP```) and M-step (```update_MP```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def report_labels(labels,side_info):\n",
    "    for l in np.unique(labels):\n",
    "        print('cluster {0:d}'.format(int(l)))\n",
    "        members = np.nonzero(labels==l)\n",
    "        tmp = np.sort(np.array(side_info)[members])\n",
    "        tmp = Counter(tmp)\n",
    "        for z in tmp.keys():\n",
    "            author = z\n",
    "            if z == '':\n",
    "                author = 'DISPUTED'\n",
    "            print( '({} article(s) by {}),'.format(tmp[z], author), end=' ' )\n",
    "        print()\n",
    "            \n",
    "def fit(xs,K,side_info):    \n",
    "    L = xs.shape[1]\n",
    "    N = xs.shape[0]\n",
    "    print( \"Fitting on data of size N: {} and L: {}\".format(N, L) )\n",
    "    best_loglik = -1e+308\n",
    "    best_logliks = []\n",
    "    best_labels = []\n",
    "    best_ls = []\n",
    "    best_pis = []\n",
    "    verbose = False\n",
    "    for s in range(100): #run the algorithm 100 times\n",
    "        np.random.seed(s)\n",
    "        ls = np.mean(xs,0)*(1.0 + 0.5*(np.random.rand(K,L)-0.5)) #each time initialize with different lambda\n",
    "        pis = [1./K]*K\n",
    "        logliks = []\n",
    "        for it in range(50): #each time run for 50 iterations\n",
    "            logqs, loglik, labels = logposterior_MP(xs,ls,pis)\n",
    "            qs = np.exp(logqs)\n",
    "            logliks.append(loglik)\n",
    "            #plt.plot(logliks)\n",
    "            ls,pis = update_MP(qs,xs)\n",
    "        if verbose:\n",
    "            print( '\\n Seed {}: log-likelihood = {:.5}, best log-likelihood so far = {:.5}'.format(s+1, loglik, best_loglik) )\n",
    "        \n",
    "        if loglik > best_loglik:\n",
    "            best_loglik = loglik    \n",
    "            print( '\\n A fit with better log-likelihood ({}) found for for seed {}'.format(loglik, s))\n",
    "            report_labels(labels,side_info)\n",
    "            best_ls = ls\n",
    "            best_pis = pis\n",
    "            best_labels = labels\n",
    "            best_logliks = logliks            \n",
    "    print('Best')\n",
    "    report_labels(best_labels,side_info)\n",
    "    return best_ls, best_pis, best_labels, best_logliks\n",
    "\n",
    "ls_MP, pis_MP, labels_MP, logliks_MP = fit(dataMat_selected, 4, [doc['authors'] for doc in documents])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model\n",
    "In this model we first re-weight our feature with a method called *tf-idf*. The main idea of *tf-idf* is that a word which appears in most of the documents, provides less information for classification/clustering; and, therefore, this word should be weighted down. <br \\>\n",
    "\n",
    "1. *tf*(n, f): term frequency(counts), in our case, it is the value of the feature(word) f in the document n.  <br \\>\n",
    "2. *df*(f): document frequency, the number of documents which contain word f. <br \\>\n",
    "3. *idf*(f): inverse document-frequency, $$\\text{idf}(f)=\\log \\frac{1+N}{1+df(f)} + 1,$$ where N is the number of samples(documents).\n",
    "4. tf-idf is defined as \n",
    "$$\n",
    "\\text{tf-idf}(n, f) = \\text{tf}(n, f)*\\text{idf}(f)\n",
    "$$\n",
    "5. Each sample after tf-idf transformation are normalized with their Euclidean norm\n",
    "$$\n",
    "x_i = \\frac{x_i}{ \\Vert {x_i} \\Vert_2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **(1pt)** Implement tf-idf computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_idf(dataMat):\n",
    "    idfVec = np.zeros((dataMat.shape[1],))\n",
    "    N = dataMat.shape[0]\n",
    "    for i in np.arange(dataMat.shape[1]):\n",
    "        df = ...\n",
    "        idfVec[i] = np.log( ... ) + ...\n",
    "    return idfVec\n",
    "test_data = np.array([[1, 0, 1, 0], [0, 1, 0, 0], [1, 1, 3, 0], [2, 3, 0, 4]])\n",
    "res_idf = compute_idf(test_data)\n",
    "assert( np.allclose(res_idf, [1.223143, 1.223143, 1.510825, 1.916290]) )\n",
    "\n",
    "\n",
    "def compute_tfidf(dataMat):\n",
    "    data_idf = compute_idf(dataMat)\n",
    "    N = dataMat.shape[0]\n",
    "    dataMat_tfidf = np.zeros( dataMat.shape, dtype='float32' )\n",
    "    for i in np.arange(N):\n",
    "        dataMat_tfidf[i, :] = dataMat[i, :]*data_idf\n",
    "        dataMat_tfidf[i, :] = dataMat_tfidf[i, :] / np.sqrt(np.sum(dataMat_tfidf[i, :]**2))\n",
    "    return data_idf, dataMat_tfidf\n",
    "# test function\n",
    "test_data = np.array([[1, 0, 1, 0], [0, 1, 0, 0], [1, 1, 3, 0], [2, 3, 0, 4]], dtype='float32')\n",
    "data_idf, res_tfidf = compute_tfidf(test_data)\n",
    "assert( np.allclose(res_tfidf, [[ 0.62922752,  0.        ,  0.77722114,  0.        ],\n",
    "       [ 0.        ,  1.        ,  0.        ,  0.        ],\n",
    "       [ 0.25212485,  0.25212485,  0.93427306,  0.        ],\n",
    "       [ 0.27662638,  0.41493958,  0.        ,  0.86677736]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now transform the data to tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_idf, dataMat_tfidf = compute_tfidf(dataMat)\n",
    "dataMat_tfidf_selected = np.asarray(dataMat_tfidf[:,lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Gaussian model\n",
    "Since the data after tf-idf transformed is no longer count data, we should our model to Gaussian.\n",
    "\n",
    "1) multivariate Gaussian pdf $$p(x\\mid\\mu, \\Sigma) = (2 \\pi)^{-\\frac{k}{2}} |\\Sigma|^{-\\frac{1}{2}} \\exp \\{ -\\frac{1}{2} {(x-\\mu)^T \\Sigma^{-1} (x-\\mu)} \\}$$  <br \\>\n",
    "In our model, we set $\\Sigma$ to be a diagonal matrix.\n",
    "\n",
    "2) Notation for the model\n",
    "  1. $x_i$ be the feature vector of the $i^{th}$ sample, $x_{i, j}$ be the $j^{th}$ features of the $i^{th}$ sample. \n",
    "  2. $h_i$ be the index of the cluster for the $i^{th}$ sample. \n",
    "  3. $\\mu_m$ be the mean vector for the $m^{th}$ cluster, $\\mu_{m,j}$ be the $j^{th}$ feature of the mean vector.\n",
    "  4. $\\Sigma_m$ be the covariance matrix for the $m^{th}$ cluster, $\\sigma^2_{m, j}$ be the $j^{th}$ variance for the $m^{th}$ cluster.\n",
    "  5. $p(h_i = c) = \\pi_c$\n",
    "  \n",
    "3) $$\n",
    "\\begin{aligned}\n",
    "\\log p(x_i\\mid \\mu_m, \\Sigma_m ) &= -\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log(|\\Sigma_m|)-\\frac{1}{2}(x_i-\\mu_m)^T\\Sigma_m^{-1}(x_i-\\mu_m) \\\\\n",
    "&= -\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\prod_j \\sigma^2_{m,j})-\\frac{1}{2} \\sum_j \\frac{(x_{i,j}-\\mu_{m,j})^2}{\\sigma_{m,j}^2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions 4-6 are equivalent to the answers you gave in mixture of Poissons. Of course, this model uses different parameters. Hence instead of $\\lambda_m$ you would write $\\mu_m$ and $\\sigma_m^2$.\n",
    "\n",
    "4) log-likelihood of mixture of Gaussian model is:\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "LL(\\mu,\\sigma) &= ... \\\\\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Apply Jensen’s inequality to derive lower-bound.\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "LL(\\mu, \\sigma^2, \\pi) = \\log( \\prod_{i} p(x_{i})) &= ...\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) If we let $q(h_i)$ be the posterior probability $p(h_i \\mid x_i, \\mu,\\sigma,\\pi)$, we maximize the lower-bound. Derive posterior formulation. \n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "p(h_i = m\\mid x_i, \\mu,\\sigma^2,\\pi) &= ...\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **(1pt)**  Write the code to compute posterior. <br \\>\n",
    "  Due to numerical precision, we compute probability in log domain. First implement the function to compute $\\log p(x_i \\mid \\mu_m,\\sigma_m^2)$ \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xs: a vector for x_i\n",
    "#mu: a vector for mu\n",
    "#sigma2: a vector for sigma square. Since we assume covariance matrix is diagonal, \n",
    "#        we can use a vector to save the non-zero values.\n",
    "#return log probability \n",
    "def logprobvec_MG(xs,mu, sigma2):\n",
    "    lp = ...\n",
    "    return lp\n",
    "\n",
    "#test function\n",
    "test_x = np.array([.2, 2.2])\n",
    "test_mu = np.array([1, 3])\n",
    "test_sigma2 = np.array([2, 0.5])\n",
    "res_v = logprobvec_MG(test_x, test_mu, test_sigma2)\n",
    "assert( np.allclose(res_v, -2.6378770) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xs, an array of shape N*F (N, # samples, F, # features)\n",
    "#mus, an array of shape K*F (K, # clusters, F, # features )\n",
    "#sigma2s, an array of shape K*F (K, # clusters, F, # features)\n",
    "#pis, a vector of shape K\n",
    "#return three varaibles:\n",
    "#logprobs: an array of shape K*N, the log posterior probabilities in 7), the log probability of a sample belong to each cluster.\n",
    "#loglik: log-likelihood of the model in 4).\n",
    "#labels: a vector of shape N, the most probable cluster each sample belongs to.\n",
    "def logposterior_MG(xs, mus, sigma2s, pis):\n",
    "    K = mus.shape[0]    \n",
    "    F = mus.shape[1]\n",
    "    N = xs.shape[0]    \n",
    "    assert(xs.shape[1] == F)\n",
    "    logprobs = np.zeros((K,N))\n",
    "    loglik = 0\n",
    "    labels = np.zeros(N)\n",
    "    for n in range(N) :\n",
    "        x = xs[n,:]\n",
    "        for k in range(K):\n",
    "            mu = mus[k,:]\n",
    "            sigma2 = sigma2s[k,:]\n",
    "            logprobs[k,n] = ...      \n",
    "        docloglik = logsum(logprobs[:,n])    \n",
    "        loglik = loglik + docloglik\n",
    "        logprobs[:,n] = ...      \n",
    "        labels[n] = np.argmax(logprobs[:,n])\n",
    "    \n",
    "    return logprobs, loglik, labels\n",
    "\n",
    "#test function\n",
    "test_x = np.array( [[1.2, .2, .1], [1, 2, 1], [.5, .6, 1]], dtype='float32' )\n",
    "test_mu = np.tile( np.round(np.mean(test_x, axis=0)), (3,1))\n",
    "np.random.seed(10)\n",
    "test_mu = test_mu + np.random.randn(test_mu.shape[0], test_mu.shape[1])\n",
    "test_sigma = np.ones(test_mu.shape)\n",
    "test_pis = np.array([0.33, 0.33, 0.33])\n",
    "res_lp, res_ll, res_lab = logposterior_MG( test_x, test_mu, test_sigma, test_pis )\n",
    "assert( np.allclose( res_lp, [[-1.91883423, -2.51793008, -3.58124658],\n",
    "       [-0.97027882, -0.72769468, -0.98950581],\n",
    "       [-0.74603191, -0.82930499, -0.51016141]]))\n",
    "assert( np.allclose( res_ll, -11.189608 ))\n",
    "assert( np.allclose( res_lab, [2, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **(1pt)**Derive the update formulation for $\\mu_{m,j}$. Take the derivative of the lower-bound.\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\mu_{m, j}} \\sum_{i} \\sum_{c} ...\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>\n",
    "Let the derivative be zero, we have\n",
    "$$\n",
    "\\mu_{m, j} = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **(1pt)** Derive the update formulation for $\\sigma^2_{m,j}$. Take the derivative on the lower-bound derived from 3).\n",
    "<br \\> <br \\> $$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\sigma^2_{m, j}} \\sum_{i} \\sum_{c} q(h_i=c) \\log \\left \\{  p(x_{i}, h_i = c) \\right \\} &=\n",
    "...\n",
    "\\end{aligned}\n",
    "$$ <br \\><br \\>\n",
    "Let the derivative be zero, we have\n",
    "$$\n",
    "\\sigma^2_{m, j} = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)  Update formulation for $\\pi_m$ is the same as mixture of Poisson model.\n",
    "<br \\><br \\>$$\n",
    "\\pi_m = ...\n",
    "$$ <br \\><br \\>\n",
    "where N is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **(1pt)** ** Write the code to update $\\mu_{m,j}$, $\\sigma^2_{m,j}$ and $\\pi_m$ ** (hint: the update rule of $\\pi_m$ is the same as mixture of Poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qs: an array of shape K*N, the posterior probabilities in 7), the probability of a sample belong to each cluster.\n",
    "#xs, an array of shape N*F (N, # samples, F, # features)\n",
    "#return two variables:\n",
    "#mus, the updated \\mu, an array of shape K*F (K, # clusters, F, # features )\n",
    "#sigma2s, the updated \\sigma^2, an array of shape K*F (K, # clusters, F, # features )\n",
    "#pis: the updated \\pi, an vector of shape K\n",
    "def update_MG(qs,xs):\n",
    "    suff = ...\n",
    "    denom = np.sum(qs,1)[:,np.newaxis]\n",
    "    mus = (suff+1e-6) / (denom+1e-6)\n",
    "    K = qs.shape[0]\n",
    "    sigma2s = np.zeros(mus.shape)\n",
    "    for i in np.arange(K):\n",
    "        suff = ...\n",
    "        sigma2s[i] = (suff+1e-6)/(np.sum(qs[i, :])+1e-6)\n",
    "    sigma2s = sigma2s+1e-6\n",
    "    pis = ...\n",
    "    pis = .../...\n",
    "    return mus, sigma2s, pis\n",
    "\n",
    "#test function\n",
    "test_qs = np.array([[ 0.14677797,  0.08062632,  0.02784097],\n",
    "       [ 0.37897736,  0.48302123,  0.37176037],\n",
    "       [ 0.47424467,  0.43635245,  0.60039866]])\n",
    "test_xs = np.array( [[1.2, .2, .1], [1, 2, 1], [.5, .6, 1]], dtype='float32' )\n",
    "res_mus, res_sigma2s, res_pis = update_MG(test_qs, test_xs)\n",
    "assert( np.allclose( res_mus, [[ 1.060471, 0.812210, 0.482459],\n",
    "       [ 0.910773, 1.025236, 0.723544],\n",
    "       [ 0.864096,0.878753, 0.717524]]))\n",
    "assert( np.allclose( res_sigma2s, [[0.04661863,  0.66609716,  0.1979422 ],\n",
    "       [ 0.07965803,  0.63567057,  0.17238403],\n",
    "       [ 0.09342444,  0.53853368,  0.17443729]]))\n",
    "assert( np.allclose( res_pis, [ 0.085081, 0.411252, 0.503665]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def fit_MG(xs,K,side_info):\n",
    "    F = xs.shape[1]\n",
    "    N = xs.shape[0]\n",
    "    print( \"Fitting on data of size N: {} and F: {}\".format(N, F) )\n",
    "    best_loglik = -1e+308\n",
    "    best_logliks = []\n",
    "    best_labels = []\n",
    "    best_mu = []\n",
    "    best_sigma2s = []\n",
    "    best_pis = []\n",
    "    verbose = False\n",
    "    for s in range(100): #run the algorithm 100 times\n",
    "        np.random.seed(s)\n",
    "        mus = np.mean(xs,0) + .01*np.random.randn(K,F) #each time initialize with different lambda\n",
    "        sigma2s = np.ones((K, F))*10\n",
    "        #sigma2s = np.tile( 10*(np.std(xs, axis=0)**2), [K, 1])\n",
    "        #sigma2s = 1./mog.precisions_\n",
    "        pis = [1./K]*K\n",
    "        #pis = mog.weights_\n",
    "        logliks = []\n",
    "        for it in range(20): #each time run for 50 iterations\n",
    "            logqs,loglik, labels = logposterior_MG(xs,mus,sigma2s, pis)\n",
    "            qs = np.exp(logqs)\n",
    "            logliks.append(loglik)\n",
    "            mus, sigma2s, pis = update_MG(qs,xs)\n",
    "        if verbose:\n",
    "            print( '\\n Seed {}: log-likelihood = {:.5}, best log-likelihood so far = {:.5}'.format(s+1, loglik, best_loglik) )\n",
    "        \n",
    "        if loglik > best_loglik:\n",
    "            best_loglik = loglik    \n",
    "            print( '\\n A fit with better log-likelihood ({}) found for for seed {}'.format(loglik, s))\n",
    "            report_labels(labels,side_info)\n",
    "            best_mus = mus\n",
    "            best_sigma2s = sigma2s\n",
    "            best_pis = pis\n",
    "            best_labels = labels\n",
    "            best_logliks = logliks\n",
    "    print('Best')\n",
    "    report_labels(best_labels,side_info)\n",
    "    return best_mus, best_sigma2s, best_pis, best_labels, best_logliks\n",
    "mus_MG, sigma2s_MG, pis_MG, labels_MG, logliks_MG = \\\n",
    "fit_MG(dataMat_tfidf_selected, 4, [doc['authors'] for doc in documents])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare two clustering results with Hypergeometric test\n",
    "\n",
    "Next we will test each cluster for enrichment in articles authored by different people (Hamilton, Madison, Jay).\n",
    "If the p-value is small, then the level of enrichment could not have occured simply by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "def gen_table( labels, documents ):\n",
    "    K = len(np.unique(labels))\n",
    "    authorList = np.asarray([doc['authors'] for doc in documents])\n",
    "    nameList = np.asarray( ['JAY', 'MADISON', 'HAMILTON'] )\n",
    "    nameList2 = np.asarray( ['JAY', 'MADISON', 'HAMILTON', ''] )\n",
    "    nameNum = {'JAY': 5, 'MADISON': 14, 'HAMILTON': 51}\n",
    "    enrichment = np.zeros((K, 3))\n",
    "    numberTab = np.zeros((K,4))\n",
    "    for i in np.arange(K):\n",
    "        cList = authorList[np.nonzero(labels==i)[0]]\n",
    "        cnt = 0\n",
    "        for j in nameList:\n",
    "            rv = hypergeom(70, nameNum[j], len(cList) )\n",
    "            cIntersect = len(np.nonzero(cList==j)[0])\n",
    "            enrichment[i, cnt] = 1-rv.cdf(cIntersect)\n",
    "            numberTab[i, cnt] = cIntersect\n",
    "            cnt = cnt + 1\n",
    "        cnt = 0\n",
    "        for j in nameList2:\n",
    "            cIntersect = len(np.nonzero(cList==j)[0])\n",
    "            numberTab[i, cnt] = cIntersect\n",
    "            cnt = cnt + 1\n",
    "    return enrichment, numberTab\n",
    "\n",
    "def report_table(enrichment, numberTab):\n",
    "    nameList = np.asarray( ['JAY', 'MADISON', 'HAMILTON'] )\n",
    "    nameList2 = np.asarray( ['JAY', 'MADISON', 'HAMILTON', 'DISPUTED'] )\n",
    "    print( 'Clustering results of Mixture of Poisson: ')\n",
    "    print( 'p-value (smaller indicates more significant enrichment): ')\n",
    "    print('\\t\\t', end=\"\")\n",
    "    for z in nameList:\n",
    "        print('{0:<8}\\t'.format(z), end='')\n",
    "    print()\n",
    "    for i in range(4):\n",
    "        print('cluster {}\\t'.format(i), end='')\n",
    "        for j in range(3):\n",
    "            print('{0:3f}\\t'.format(enrichment[i, j]), end='')\n",
    "        print()\n",
    "\n",
    "    print( '\\n\\ncount table: ')\n",
    "    print('\\t\\t', end=\"\")\n",
    "    for z in nameList2:\n",
    "        print('{0:<8}\\t'.format(z), end='')\n",
    "    print()\n",
    "    for i in range(4):\n",
    "        print('cluster {}\\t'.format(i), end='')\n",
    "        for j in range(4):\n",
    "            print('{0:3f}\\t'.format(numberTab[i, j]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrichment, numberTab = gen_table( labels_MP, documents)\n",
    "report_table(enrichment,numberTab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrichment, numberTab = gen_table( labels_MG, documents)\n",
    "report_table(enrichment,numberTab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) **(1pt)** State your conclusion about who is most likely to have authored disputed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
